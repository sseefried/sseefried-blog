% Scripting your code generation pipeline to produce efficient code from high-level languages
% Sean Seefried
% Fri 07 Jan 2022

What if it's true that the fancy libraries written in functional programming languages are just too darn abstract to have any chance of compiling down to fast machine code?

What if it's true that the reason for this is not that optimising compilers are not sophisticated enough but the flawed idea idea that it's possible to write an optimising compiler that "does the best thing" no matter what program you throw at?

C compilers do a pretty good job of producing very efficient code but C is not a particularly high-level language. Perhaps _this_ is the reason that a C compiler that produces very high performance code, for a wide variety of programs, is even possible.

But how true is this in the land of functional programming? In Haskell, thanks to its powers of abstraction, it is possible to build high-level, composable abstractions that are a joy to work with but don't necessarily compile down to efficient machine code.
Interestingly one way of redressing this problem in GHC is through the use of REWRITE rules which is a way of teaching the compiler things it didn't know before.

An radical extension of this idea is what I think may be necessary for functional programming to truly show its full colours. In the Formal Methods space there is a concept known as _refinement_. In a nutshell, one takes a high level specification and an implementation of that specification and show that the implementation is "faithful" to the specification. I won't go into the details of what faithful means here.

The refined implementation is free to introduce various operational concerns such as time and space which may not be, and probably aren't, present in the specification. It is the cleverness with which the refinements are done that determines how well the implementation performs.

One can think of the compiler as a series of refinements down to the eventual machine code that is produced. Yet, for a typical compiler, this series of refinements is _fixed_. The user of the compiler has no control over the order in which the refinements occur or which ones are applied. In the following discussion I will restrict us to examining just the (machine) code generation pipeline (skipping all mention of earlier phases of the compiler such as parsing or type checking).

A significant part of the code generation pipeline is optimisations.
Typically an optimisation will only ever make it in into a compiler if it can be shown to work on a wide array of different programs. Optimisations that only work in narrow programming domains will simply not make the cut.

I propose that we change the fixed nature of the code generation pipeline in two main ways.

First we allow people to script it. They can choose which phases of the code generation pipeline are used and in which order. Perhaps there are even multiple ways of generating machine code which users can choose from.

Second, we allow optimisations that are only valid if one has proved certain properties about one's program. Thus, proof not only provides us with a gurantee of correctness, but also helps us produce more efficient code under certain circumstances.

The next question is, how should one interact with the code generation pipeline? In order for users to script and provide proofs to the pipeline it seems that one needs to be able to pass flags to the compiler or perhaps even write plugins that the compiler will run during its execution.

However, there is another possibility.

One could use the code generation pipeline _as a library_. This gives a user the ability to script the code generation pipeline _for free_.

However, this does entail running the code generation pipeline in an entirely new manner. Normally for a given program `P` one simply passes it into compiler `C` as input, and receives executable/object-file `O` as output.

However, in this new setup one actually writes a program `P'` slightly different to program `P`. It is mostly like program `P` except that this program must then be passed into a code generation function. For now I will gloss over how this is done but this will be addressed shortly.

One then compiles program `P'` using compile `C` which produces an output `O'` _which then needs to be run_ to produce the final executable/object-file `O`. An extra stage has been added to compilation under this scheme.

There is also another technicality that needs to be addressed. Presumably `O'` is itself an executable file. How was the machine code generated by `C` if the code generation pipeline is a library? Superficially it seems like we have a problem here but this is just the age old problem of bootstrapping which has been solved. Compiler `C` can simply use a default code generation pipeline to produce executable `O'`.

I will now address the question of just how the program is passed into the code generation function exposed by the code generation library. Normally the code generation pipeline of the compiler takes an Abstract Syntax Tree (AST) or some other intermediate language representation as input. How can we do this when we use the code generator as a library?

In truth, this part of the proposal is the most fuzzy to me and it may have a better answer than the one I'm not going to provide, which is this: simply expose a built-in language function which, given a program, returns its AST. This would not be too different to various _reflection_ or _meta-programming_ facilities provided by  modern languages.

The next objection to this proposal is that it may be a lot of work for a programmer to script the code generation pipeline. However, this is completely optional. One can simply use the default code generation pipeline at the expense of producing less efficient code.

This proposal is founded on the assumption that there _may not be_ a general optimising compiler that can produce efficient code for all high level programs. This assumption may prove to be false but I think there are good reasons to doubt this.

One of the advantages of programming in a high level language is that one is free to use abstractions far removed from what the underlying hardware architecture and operating system provides. One is free to focus on simplicity, understandability, correctness, and so on.

However in the current state of affairs programmers must also be able to reason about space and efficiency of their programs. This entails knowing at least something about how the code generator of the compiler works, and at least something about the machine instructions supported by the underlying hardware.

I posit that the reason that compilers for languages like C perform well is because the level of abstraction is not very high. The code is already fairly "close to the machine" so-to-speak. Further, one can easily reason about space and efficiency because of this.

However for higher level languages this is less true. The basic constructs of functional programming languages are "further from the machine". Take infinite precision integer arithmetic for instance.
There are a wide array of different programming problem domains e.g. computer graphics, signal processing, numeric computing, machine learning, to name just a few. Given this diversity of programming domains, and the diversity of hardware suited to implementing them efficiently does it even make sense to speak of a single code generation pipeline that can handle all of these kinds of programs effectively?

-----
## Further notes

Programmers are given very little _control_ and very little _visibility_ into this process. We have already addressed control, but what do I mean by visibility? Simply this. One cannot easily (if at all) see the data structures produced during the intermediate stages of the code generation pipeline. This is also redressed by using the code generator as a library.
